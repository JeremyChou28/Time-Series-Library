Args in experiment:
[1mBasic Config[0m
  Task Name:          classification      Is Training:        1                   
  Model ID:           EthanolConcentrationModel:              Transformer         

[1mData Loader[0m
  Data:               UEA                 Root Path:          ../iTransformer_datasets/classification/EthanolConcentration/
  Data Path:          ETTh1.csv           Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Top k:              3                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            128                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        0                   Itr:                1                   
  Train Epochs:       100                 Batch Size:         16                  
  Patience:           10                  Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
261
263
>>>>>>>start training : classification_EthanolConcentration_Transformer_UEA_ftM_sl1751_ll48_pl0_dm128_nh8_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
261
263
263
Traceback (most recent call last):
  File "run.py", line 272, in <module>
    exp.train(setting)
  File "/Jianping/Research/Time-Series-Library/exp/exp_classification.py", line 110, in train
    outputs = self.model(batch_x, padding_mask, None, None)
  File "/opt/conda/envs/torch1.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/Jianping/Research/Time-Series-Library/models/Transformer.py", line 123, in forward
    dec_out = self.classification(x_enc, x_mark_enc)
  File "/Jianping/Research/Time-Series-Library/models/Transformer.py", line 102, in classification
    enc_out, attns = self.encoder(enc_out, attn_mask=None)
  File "/opt/conda/envs/torch1.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/Jianping/Research/Time-Series-Library/layers/Transformer_EncDec.py", line 74, in forward
    x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)
  File "/opt/conda/envs/torch1.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/Jianping/Research/Time-Series-Library/layers/Transformer_EncDec.py", line 40, in forward
    new_x, attn = self.attention(
  File "/opt/conda/envs/torch1.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/Jianping/Research/Time-Series-Library/layers/SelfAttention_Family.py", line 203, in forward
    out, attn = self.inner_attention(
  File "/opt/conda/envs/torch1.8/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/Jianping/Research/Time-Series-Library/layers/SelfAttention_Family.py", line 61, in forward
    scores = torch.einsum("blhe,bshe->bhls", queries, keys)
  File "/opt/conda/envs/torch1.8/lib/python3.8/site-packages/torch/functional.py", line 408, in einsum
    return _VF.einsum(equation, operands)  # type: ignore
RuntimeError: CUDA out of memory. Tried to allocate 1.46 GiB (GPU 0; 23.69 GiB total capacity; 137.37 MiB already allocated; 201.19 MiB free; 142.00 MiB reserved in total by PyTorch)
