Args in experiment:
[1mBasic Config[0m
  Task Name:          classification      Is Training:        1                   
  Model ID:           JapaneseVowels      Model:              Reformer            

[1mData Loader[0m
  Data:               UEA                 Root Path:          ../iTransformer_datasets/classification/JapaneseVowels/
  Data Path:          ETTh1.csv           Features:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mModel Parameters[0m
  Top k:              3                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            128                 
  n heads:            8                   e layers:           3                   
  d layers:           1                   d FF:               256                 
  Moving Avg:         25                  Factor:             1                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        0                   Itr:                1                   
  Train Epochs:       100                 Batch Size:         16                  
  Patience:           10                  Learning Rate:      0.001               
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
270
370
>>>>>>>start training : classification_JapaneseVowels_Reformer_UEA_ftM_sl29_ll48_pl0_dm128_nh8_el3_dl1_df256_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
270
370
370
Traceback (most recent call last):
  File "run.py", line 272, in <module>
    exp.train(setting)
  File "/mnt/nas/home/cilab/Jianping/Research/Time-Series-Library/exp/exp_classification.py", line 122, in train
    loss.backward()
  File "/home/cilab/miniconda3/envs/zjp/lib/python3.8/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/home/cilab/miniconda3/envs/zjp/lib/python3.8/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 3.31 MiB is free. Process 1295198 has 1.04 GiB memory in use. Including non-PyTorch memory, this process has 1.49 GiB memory in use. Process 1295199 has 804.00 MiB memory in use. Process 1295194 has 1.94 GiB memory in use. Process 1295201 has 2.01 GiB memory in use. Process 1295192 has 4.46 GiB memory in use. Of the allocated memory 52.36 MiB is allocated by PyTorch, and 15.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
