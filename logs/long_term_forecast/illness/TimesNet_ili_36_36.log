Args in experiment:
[1mBasic Config[0m
  Task Name:          long_term_forecast  Is Training:        1                   
  Model ID:           ili_36_36           Model:              TimesNet            

[1mData Loader[0m
  Data:               custom              Root Path:          ../iTransformer_datasets/illness/
  Data Path:          national_illness.csvFeatures:           M                   
  Target:             OT                  Freq:               h                   
  Checkpoints:        ./checkpoints/      

[1mForecasting Task[0m
  Seq Len:            36                  Label Len:          18                  
  Pred Len:           36                  Seasonal Patterns:  Monthly             
  Inverse:            0                   

[1mModel Parameters[0m
  Top k:              5                   Num Kernels:        6                   
  Enc In:             7                   Dec In:             7                   
  C Out:              7                   d model:            768                 
  n heads:            8                   e layers:           2                   
  d layers:           1                   d FF:               768                 
  Moving Avg:         25                  Factor:             3                   
  Distil:             1                   Dropout:            0.1                 
  Embed:              timeF               Activation:         gelu                
  Output Attention:   0                   

[1mRun Parameters[0m
  Num Workers:        0                   Itr:                1                   
  Train Epochs:       10                  Batch Size:         32                  
  Patience:           3                   Learning Rate:      0.0001              
  Des:                Exp                 Loss:               MSE                 
  Lradj:              type1               Use Amp:            0                   

[1mGPU[0m
  Use GPU:            1                   GPU:                0                   
  Use Multi GPU:      0                   Devices:            0,1,2,3             

[1mDe-stationary Projector Params[0m
  P Hidden Dims:      128, 128            P Hidden Layers:    2                   

Use GPU: cuda:0
>>>>>>>start training : long_term_forecast_ili_36_36_TimesNet_custom_ftM_sl36_ll18_pl36_dm768_nh8_el2_dl1_df768_fc3_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>
train 605
val 62
test 158
Traceback (most recent call last):
  File "run.py", line 272, in <module>
    exp.train(setting)
  File "/mnt/nas/home/cilab/Jianping/Research/Time-Series-Library/exp/exp_long_term_forecasting.py", line 157, in train
    model_optim.step()
  File "/home/cilab/miniconda3/envs/zjp/lib/python3.8/site-packages/torch/optim/optimizer.py", line 385, in wrapper
    out = func(*args, **kwargs)
  File "/home/cilab/miniconda3/envs/zjp/lib/python3.8/site-packages/torch/optim/optimizer.py", line 76, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/cilab/miniconda3/envs/zjp/lib/python3.8/site-packages/torch/optim/adam.py", line 157, in step
    has_complex = self._init_group(
  File "/home/cilab/miniconda3/envs/zjp/lib/python3.8/site-packages/torch/optim/adam.py", line 111, in _init_group
    state['exp_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 274.00 MiB. GPU 0 has a total capacity of 11.76 GiB of which 96.19 MiB is free. Including non-PyTorch memory, this process has 11.52 GiB memory in use. Of the allocated memory 9.55 GiB is allocated by PyTorch, and 504.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
