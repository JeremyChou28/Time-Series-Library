{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
    "from exp.exp_imputation import Exp_Imputation\n",
    "from exp.exp_short_term_forecasting import Exp_Short_Term_Forecast\n",
    "from exp.exp_anomaly_detection import Exp_Anomaly_Detection\n",
    "from exp.exp_classification import Exp_Classification\n",
    "from utils.print_args import print_args\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    fix_seed = 2021\n",
    "    random.seed(fix_seed)\n",
    "    torch.manual_seed(fix_seed)\n",
    "    np.random.seed(fix_seed)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"TimesNet\")\n",
    "\n",
    "    # basic config\n",
    "    parser.add_argument(\n",
    "        \"--task_name\",\n",
    "        type=str,\n",
    "        default=\"long_term_forecast\",\n",
    "        help=\"task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]\",\n",
    "    )\n",
    "    parser.add_argument(\"--is_training\", type=int, default=1, help=\"status\")\n",
    "    parser.add_argument(\"--model_id\", type=str, default=\"ETTh1_96_96\", help=\"model id\")\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        default=\"MICN\",\n",
    "        help=\"model name, options: [Autoformer, Transformer, TimesNet]\",\n",
    "    )\n",
    "\n",
    "    # data loader\n",
    "    parser.add_argument(\"--data\", type=str, default=\"ETTh1\", help=\"dataset type\")\n",
    "    parser.add_argument(\n",
    "        \"--root_path\",\n",
    "        type=str,\n",
    "        default=\"../iTransformer_datasets/ETT-small/\",\n",
    "        help=\"root path of the data file\",\n",
    "    )\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"ETTh1.csv\", help=\"data file\")\n",
    "    parser.add_argument(\n",
    "        \"--features\",\n",
    "        type=str,\n",
    "        default=\"M\",\n",
    "        help=\"forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target\", type=str, default=\"OT\", help=\"target feature in S or MS task\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--freq\",\n",
    "        type=str,\n",
    "        default=\"h\",\n",
    "        help=\"freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoints\",\n",
    "        type=str,\n",
    "        default=\"./checkpoints/\",\n",
    "        help=\"location of model checkpoints\",\n",
    "    )\n",
    "\n",
    "    # forecasting task\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=96, help=\"input sequence length\")\n",
    "    parser.add_argument(\"--label_len\", type=int, default=96, help=\"start token length\")\n",
    "    parser.add_argument(\n",
    "        \"--pred_len\", type=int, default=96, help=\"prediction sequence length\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seasonal_patterns\", type=str, default=\"Monthly\", help=\"subset for M4\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--inverse\", action=\"store_true\", help=\"inverse output data\", default=False\n",
    "    )\n",
    "\n",
    "    # inputation task\n",
    "    parser.add_argument(\"--mask_rate\", type=float, default=0.25, help=\"mask ratio\")\n",
    "\n",
    "    # anomaly detection task\n",
    "    parser.add_argument(\n",
    "        \"--anomaly_ratio\", type=float, default=0.25, help=\"prior anomaly ratio (%)\"\n",
    "    )\n",
    "\n",
    "    # model define\n",
    "    parser.add_argument(\"--top_k\", type=int, default=5, help=\"for TimesBlock\")\n",
    "    parser.add_argument(\"--num_kernels\", type=int, default=6, help=\"for Inception\")\n",
    "    parser.add_argument(\"--enc_in\", type=int, default=7, help=\"encoder input size\")\n",
    "    parser.add_argument(\"--dec_in\", type=int, default=7, help=\"decoder input size\")\n",
    "    parser.add_argument(\"--c_out\", type=int, default=7, help=\"output size\")\n",
    "    parser.add_argument(\"--d_model\", type=int, default=512, help=\"dimension of model\")\n",
    "    parser.add_argument(\"--n_heads\", type=int, default=8, help=\"num of heads\")\n",
    "    parser.add_argument(\"--e_layers\", type=int, default=2, help=\"num of encoder layers\")\n",
    "    parser.add_argument(\"--d_layers\", type=int, default=1, help=\"num of decoder layers\")\n",
    "    parser.add_argument(\"--d_ff\", type=int, default=2048, help=\"dimension of fcn\")\n",
    "    parser.add_argument(\n",
    "        \"--moving_avg\", type=int, default=25, help=\"window size of moving average\"\n",
    "    )\n",
    "    parser.add_argument(\"--factor\", type=int, default=3, help=\"attn factor\")\n",
    "    parser.add_argument(\n",
    "        \"--distil\",\n",
    "        action=\"store_false\",\n",
    "        help=\"whether to use distilling in encoder, using this argument means not using distilling\",\n",
    "        default=True,\n",
    "    )\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.1, help=\"dropout\")\n",
    "    parser.add_argument(\n",
    "        \"--embed\",\n",
    "        type=str,\n",
    "        default=\"timeF\",\n",
    "        help=\"time features encoding, options:[timeF, fixed, learned]\",\n",
    "    )\n",
    "    parser.add_argument(\"--activation\", type=str, default=\"gelu\", help=\"activation\")\n",
    "    parser.add_argument(\n",
    "        \"--output_attention\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether to output attention in ecoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--channel_independence\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"0: channel dependence 1: channel independence for FreTS model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--decomp_method\",\n",
    "        type=str,\n",
    "        default=\"moving_avg\",\n",
    "        help=\"method of series decompsition, only support moving_avg or dft_decomp\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_norm\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"whether to use normalize; True 1 False 0\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--down_sampling_layers\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"num of down sampling layers\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--down_sampling_window\", type=int, default=1, help=\"down sampling window size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--down_sampling_method\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"down sampling method, only support avg, max, conv\",\n",
    "    )\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument(\n",
    "        \"--num_workers\", type=int, default=0, help=\"data loader num workers\"\n",
    "    )\n",
    "    parser.add_argument(\"--itr\", type=int, default=1, help=\"experiments times\")\n",
    "    parser.add_argument(\"--train_epochs\", type=int, default=10, help=\"train epochs\")\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=32, help=\"batch size of train input data\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--patience\", type=int, default=3, help=\"early stopping patience\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", type=float, default=0.0001, help=\"optimizer learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\"--des\", type=str, default=\"Exp\", help=\"exp description\")\n",
    "    parser.add_argument(\"--loss\", type=str, default=\"MSE\", help=\"loss function\")\n",
    "    parser.add_argument(\n",
    "        \"--lradj\", type=str, default=\"type1\", help=\"adjust learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_amp\",\n",
    "        action=\"store_true\",\n",
    "        help=\"use automatic mixed precision training\",\n",
    "        default=False,\n",
    "    )\n",
    "\n",
    "    # GPU\n",
    "    parser.add_argument(\"--use_gpu\", type=bool, default=True, help=\"use gpu\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0, help=\"gpu\")\n",
    "    parser.add_argument(\n",
    "        \"--use_multi_gpu\", action=\"store_true\", help=\"use multiple gpus\", default=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--devices\", type=str, default=\"0,1,2,3\", help=\"device ids of multile gpus\"\n",
    "    )\n",
    "\n",
    "    # de-stationary projector params\n",
    "    parser.add_argument(\n",
    "        \"--p_hidden_dims\",\n",
    "        type=int,\n",
    "        nargs=\"+\",\n",
    "        default=[128, 128],\n",
    "        help=\"hidden layer dimensions of projector (List)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--p_hidden_layers\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"number of hidden layers in projector\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "    if args.use_gpu and args.use_multi_gpu:\n",
    "        args.devices = args.devices.replace(\" \", \"\")\n",
    "        device_ids = args.devices.split(\",\")\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "\n",
    "    print(\"Args in experiment:\")\n",
    "    print_args(args)\n",
    "\n",
    "    if args.task_name == \"long_term_forecast\":\n",
    "        Exp = Exp_Long_Term_Forecast\n",
    "    elif args.task_name == \"short_term_forecast\":\n",
    "        Exp = Exp_Short_Term_Forecast\n",
    "    elif args.task_name == \"imputation\":\n",
    "        Exp = Exp_Imputation\n",
    "    elif args.task_name == \"anomaly_detection\":\n",
    "        Exp = Exp_Anomaly_Detection\n",
    "    elif args.task_name == \"classification\":\n",
    "        Exp = Exp_Classification\n",
    "    else:\n",
    "        Exp = Exp_Long_Term_Forecast\n",
    "\n",
    "    if args.is_training:\n",
    "        for ii in range(args.itr):\n",
    "            # setting record of experiments\n",
    "            exp = Exp(args)  # set experiments\n",
    "            setting = \"{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}\".format(\n",
    "                args.task_name,\n",
    "                args.model_id,\n",
    "                args.model,\n",
    "                args.data,\n",
    "                args.features,\n",
    "                args.seq_len,\n",
    "                args.label_len,\n",
    "                args.pred_len,\n",
    "                args.d_model,\n",
    "                args.n_heads,\n",
    "                args.e_layers,\n",
    "                args.d_layers,\n",
    "                args.d_ff,\n",
    "                args.factor,\n",
    "                args.embed,\n",
    "                args.distil,\n",
    "                args.des,\n",
    "                ii,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \">>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>\".format(setting)\n",
    "            )\n",
    "            exp.train(setting)\n",
    "\n",
    "            print(\n",
    "                \">>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\".format(setting)\n",
    "            )\n",
    "            exp.test(setting)\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        ii = 0\n",
    "        setting = \"{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}\".format(\n",
    "            args.task_name,\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des,\n",
    "            ii,\n",
    "        )\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print(\">>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\".format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Spend Time: {}\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d parameters count: 1792\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def count_parameters(conv2d_layer):\n",
    "    return sum(p.numel() for p in conv2d_layer.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# ç¤ºä¾‹\n",
    "conv = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "parameters_count = count_parameters(conv)\n",
    "print(\"Conv2d parameters count:\", parameters_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
