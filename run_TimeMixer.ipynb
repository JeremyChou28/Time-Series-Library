{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Args in experiment:\n",
      "\u001b[1mBasic Config\u001b[0m\n",
      "  Task Name:          long_term_forecast  Is Training:        1                   \n",
      "  Model ID:           ETTh1_96_96         Model:              TimeMixer           \n",
      "\n",
      "\u001b[1mData Loader\u001b[0m\n",
      "  Data:               ETTh1               Root Path:          ../iTransformer_datasets/ETT-small/\n",
      "  Data Path:          ETTh1.csv           Features:           M                   \n",
      "  Target:             OT                  Freq:               h                   \n",
      "  Checkpoints:        ./checkpoints/      \n",
      "\n",
      "\u001b[1mForecasting Task\u001b[0m\n",
      "  Seq Len:            96                  Label Len:          0                   \n",
      "  Pred Len:           96                  Seasonal Patterns:  Monthly             \n",
      "  Inverse:            0                   \n",
      "\n",
      "\u001b[1mModel Parameters\u001b[0m\n",
      "  Top k:              5                   Num Kernels:        6                   \n",
      "  Enc In:             7                   Dec In:             7                   \n",
      "  C Out:              7                   d model:            16                  \n",
      "  n heads:            8                   e layers:           2                   \n",
      "  d layers:           1                   d FF:               32                  \n",
      "  Moving Avg:         25                  Factor:             1                   \n",
      "  Distil:             1                   Dropout:            0.1                 \n",
      "  Embed:              timeF               Activation:         gelu                \n",
      "  Output Attention:   0                   \n",
      "\n",
      "\u001b[1mRun Parameters\u001b[0m\n",
      "  Num Workers:        0                   Itr:                1                   \n",
      "  Train Epochs:       10                  Batch Size:         16                  \n",
      "  Patience:           10                  Learning Rate:      0.01                \n",
      "  Des:                Exp                 Loss:               MSE                 \n",
      "  Lradj:              type1               Use Amp:            0                   \n",
      "\n",
      "\u001b[1mGPU\u001b[0m\n",
      "  Use GPU:            1                   GPU:                0                   \n",
      "  Use Multi GPU:      0                   Devices:            0                   \n",
      "\n",
      "\u001b[1mDe-stationary Projector Params\u001b[0m\n",
      "  P Hidden Dims:      128, 128            P Hidden Layers:    2                   \n",
      "\n",
      "Use GPU: cuda:0\n",
      ">>>>>>>start training : long_term_forecast_ETTh1_96_96_TimeMixer_ETTh1_ftM_sl96_ll0_pl96_dm16_nh8_el2_dl1_df32_fc1_ebtimeF_dtTrue_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 8449\n",
      "val 2785\n",
      "test 2785\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 270\u001b[0m\n\u001b[1;32m    246\u001b[0m setting \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ft\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_sl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ll\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dm\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_nh\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_el\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_df\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_fc\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_eb\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dt\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    247\u001b[0m     args\u001b[38;5;241m.\u001b[39mtask_name,\n\u001b[1;32m    248\u001b[0m     args\u001b[38;5;241m.\u001b[39mmodel_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     ii,\n\u001b[1;32m    265\u001b[0m )\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting)\n\u001b[1;32m    269\u001b[0m )\n\u001b[0;32m--> 270\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msetting\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>>>>>>>testing : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting)\n\u001b[1;32m    274\u001b[0m )\n\u001b[1;32m    275\u001b[0m exp\u001b[38;5;241m.\u001b[39mtest(setting)\n",
      "File \u001b[0;32m/Jianping/Research/Time-Series-Library/exp/exp_long_term_forecasting.py:156\u001b[0m, in \u001b[0;36mExp_Long_Term_Forecast.train\u001b[0;34m(self, setting)\u001b[0m\n\u001b[1;32m    154\u001b[0m         scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         model_optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m cost time: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m epoch_time))\n",
      "File \u001b[0;32m/opt/conda/envs/zjp/lib/python3.8/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/zjp/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_long_term_forecasting import Exp_Long_Term_Forecast\n",
    "from exp.exp_imputation import Exp_Imputation\n",
    "from exp.exp_short_term_forecasting import Exp_Short_Term_Forecast\n",
    "from exp.exp_anomaly_detection import Exp_Anomaly_Detection\n",
    "from exp.exp_classification import Exp_Classification\n",
    "from utils.print_args import print_args\n",
    "import random\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    fix_seed = 2021\n",
    "    random.seed(fix_seed)\n",
    "    torch.manual_seed(fix_seed)\n",
    "    np.random.seed(fix_seed)\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"TimesNet\")\n",
    "\n",
    "    # basic config\n",
    "    parser.add_argument(\n",
    "        \"--task_name\",\n",
    "        type=str,\n",
    "        default=\"long_term_forecast\",\n",
    "        help=\"task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--is_training\", type=int, default=1, help=\"status\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_id\", type=str, default=\"ETTh1_96_96\", help=\"model id\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        default=\"TimeMixer\",\n",
    "        help=\"model name, options: [Autoformer, Transformer, TimesNet]\",\n",
    "    )\n",
    "\n",
    "    # data loader\n",
    "    parser.add_argument(\n",
    "        \"--data\", type=str, default=\"ETTh1\", help=\"dataset type\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--root_path\",\n",
    "        type=str,\n",
    "        default=\"../iTransformer_datasets/ETT-small/\",\n",
    "        help=\"root path of the data file\",\n",
    "    )\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"ETTh1.csv\", help=\"data file\")\n",
    "    parser.add_argument(\n",
    "        \"--features\",\n",
    "        type=str,\n",
    "        default=\"M\",\n",
    "        help=\"forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target\", type=str, default=\"OT\", help=\"target feature in S or MS task\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--freq\",\n",
    "        type=str,\n",
    "        default=\"h\",\n",
    "        help=\"freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoints\",\n",
    "        type=str,\n",
    "        default=\"./checkpoints/\",\n",
    "        help=\"location of model checkpoints\",\n",
    "    )\n",
    "\n",
    "    # forecasting task\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=96, help=\"input sequence length\")\n",
    "    parser.add_argument(\"--label_len\", type=int, default=0, help=\"start token length\")\n",
    "    parser.add_argument(\n",
    "        \"--pred_len\", type=int, default=96, help=\"prediction sequence length\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seasonal_patterns\", type=str, default=\"Monthly\", help=\"subset for M4\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--inverse\", action=\"store_true\", help=\"inverse output data\", default=False\n",
    "    )\n",
    "\n",
    "    # inputation task\n",
    "    parser.add_argument(\"--mask_rate\", type=float, default=0.25, help=\"mask ratio\")\n",
    "\n",
    "    # anomaly detection task\n",
    "    parser.add_argument(\n",
    "        \"--anomaly_ratio\", type=float, default=0.25, help=\"prior anomaly ratio (%)\"\n",
    "    )\n",
    "\n",
    "    # model define\n",
    "    parser.add_argument(\"--top_k\", type=int, default=5, help=\"for TimesBlock\")\n",
    "    parser.add_argument(\"--num_kernels\", type=int, default=6, help=\"for Inception\")\n",
    "    parser.add_argument(\"--enc_in\", type=int, default=7, help=\"encoder input size\")\n",
    "    parser.add_argument(\"--dec_in\", type=int, default=7, help=\"decoder input size\")\n",
    "    parser.add_argument(\"--c_out\", type=int, default=7, help=\"output size\")\n",
    "    parser.add_argument(\"--d_model\", type=int, default=16, help=\"dimension of model\")\n",
    "    parser.add_argument(\"--n_heads\", type=int, default=8, help=\"num of heads\")\n",
    "    parser.add_argument(\"--e_layers\", type=int, default=2, help=\"num of encoder layers\")\n",
    "    parser.add_argument(\"--d_layers\", type=int, default=1, help=\"num of decoder layers\")\n",
    "    parser.add_argument(\"--d_ff\", type=int, default=32, help=\"dimension of fcn\")\n",
    "    parser.add_argument(\n",
    "        \"--moving_avg\", type=int, default=25, help=\"window size of moving average\"\n",
    "    )\n",
    "    parser.add_argument(\"--factor\", type=int, default=1, help=\"attn factor\")\n",
    "    parser.add_argument(\n",
    "        \"--distil\",\n",
    "        action=\"store_false\",\n",
    "        help=\"whether to use distilling in encoder, using this argument means not using distilling\",\n",
    "        default=True,\n",
    "    )\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.1, help=\"dropout\")\n",
    "    parser.add_argument(\n",
    "        \"--embed\",\n",
    "        type=str,\n",
    "        default=\"timeF\",\n",
    "        help=\"time features encoding, options:[timeF, fixed, learned]\",\n",
    "    )\n",
    "    parser.add_argument(\"--activation\", type=str, default=\"gelu\", help=\"activation\")\n",
    "    parser.add_argument(\n",
    "        \"--output_attention\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether to output attention in ecoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--channel_independence\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"0: channel dependence 1: channel independence for FreTS model\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--decomp_method\",\n",
    "        type=str,\n",
    "        default=\"moving_avg\",\n",
    "        help=\"method of series decompsition, only support moving_avg or dft_decomp\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_norm\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"whether to use normalize; True 1 False 0\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--down_sampling_layers\",\n",
    "        type=int,\n",
    "        default=3,\n",
    "        help=\"num of down sampling layers\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--down_sampling_window\", type=int, default=2, help=\"down sampling window size\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--down_sampling_method\",\n",
    "        type=str,\n",
    "        default='avg',\n",
    "        help=\"down sampling method, only support avg, max, conv\",\n",
    "    )\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument(\n",
    "        \"--num_workers\", type=int, default=0, help=\"data loader num workers\"\n",
    "    )\n",
    "    parser.add_argument(\"--itr\", type=int, default=1, help=\"experiments times\")\n",
    "    parser.add_argument(\"--train_epochs\", type=int, default=10, help=\"train epochs\")\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=16, help=\"batch size of train input data\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--patience\", type=int, default=10, help=\"early stopping patience\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", type=float, default=0.01, help=\"optimizer learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\"--des\", type=str, default=\"Exp\", help=\"exp description\")\n",
    "    parser.add_argument(\"--loss\", type=str, default=\"MSE\", help=\"loss function\")\n",
    "    parser.add_argument(\n",
    "        \"--lradj\", type=str, default=\"type1\", help=\"adjust learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_amp\",\n",
    "        action=\"store_true\",\n",
    "        help=\"use automatic mixed precision training\",\n",
    "        default=False,\n",
    "    )\n",
    "\n",
    "    # GPU\n",
    "    parser.add_argument(\"--use_gpu\", type=bool, default=True, help=\"use gpu\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0, help=\"gpu\")\n",
    "    parser.add_argument(\n",
    "        \"--use_multi_gpu\", action=\"store_true\", help=\"use multiple gpus\", default=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--devices\", type=str, default=\"0\", help=\"device ids of multile gpus\"\n",
    "    )\n",
    "\n",
    "    # de-stationary projector params\n",
    "    parser.add_argument(\n",
    "        \"--p_hidden_dims\",\n",
    "        type=int,\n",
    "        nargs=\"+\",\n",
    "        default=[128, 128],\n",
    "        help=\"hidden layer dimensions of projector (List)\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--p_hidden_layers\",\n",
    "        type=int,\n",
    "        default=2,\n",
    "        help=\"number of hidden layers in projector\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(args=[])\n",
    "    args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "    if args.use_gpu and args.use_multi_gpu:\n",
    "        args.devices = args.devices.replace(\" \", \"\")\n",
    "        device_ids = args.devices.split(\",\")\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "\n",
    "    print(\"Args in experiment:\")\n",
    "    print_args(args)\n",
    "\n",
    "    if args.task_name == \"long_term_forecast\":\n",
    "        Exp = Exp_Long_Term_Forecast\n",
    "    elif args.task_name == \"short_term_forecast\":\n",
    "        Exp = Exp_Short_Term_Forecast\n",
    "    elif args.task_name == \"imputation\":\n",
    "        Exp = Exp_Imputation\n",
    "    elif args.task_name == \"anomaly_detection\":\n",
    "        Exp = Exp_Anomaly_Detection\n",
    "    elif args.task_name == \"classification\":\n",
    "        Exp = Exp_Classification\n",
    "    else:\n",
    "        Exp = Exp_Long_Term_Forecast\n",
    "\n",
    "    if args.is_training:\n",
    "        for ii in range(args.itr):\n",
    "            # setting record of experiments\n",
    "            exp = Exp(args)  # set experiments\n",
    "            setting = \"{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}\".format(\n",
    "                args.task_name,\n",
    "                args.model_id,\n",
    "                args.model,\n",
    "                args.data,\n",
    "                args.features,\n",
    "                args.seq_len,\n",
    "                args.label_len,\n",
    "                args.pred_len,\n",
    "                args.d_model,\n",
    "                args.n_heads,\n",
    "                args.e_layers,\n",
    "                args.d_layers,\n",
    "                args.d_ff,\n",
    "                args.factor,\n",
    "                args.embed,\n",
    "                args.distil,\n",
    "                args.des,\n",
    "                ii,\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                \">>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>\".format(setting)\n",
    "            )\n",
    "            exp.train(setting)\n",
    "\n",
    "            print(\n",
    "                \">>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\".format(setting)\n",
    "            )\n",
    "            exp.test(setting)\n",
    "            torch.cuda.empty_cache()\n",
    "    else:\n",
    "        ii = 0\n",
    "        setting = \"{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}\".format(\n",
    "            args.task_name,\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des,\n",
    "            ii,\n",
    "        )\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print(\">>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\".format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"Spend Time: {}\".format(time.time() - start_time))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zjp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
